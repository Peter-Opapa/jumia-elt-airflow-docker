
# ğŸ›’ Jumia Laptop ELT Pipeline (Airflow + Docker + PostgreSQL)

A production-grade ELT pipeline that **scrapes laptop data** from the [Jumia Kenya](https://www.jumia.co.ke/) website, loads it into a **PostgreSQL-based data warehouse**, transforms it using **stored procedures**, and orchestrates the process using **Apache Airflow** in a **Dockerized environment**. The pipeline runs **daily** and can also be triggered manually via the Airflow web UI.

---

## ğŸ§­ Table of Contents

- [Project Overview](#project-overview)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Features](#features)
- [Project Structure](#project-structure)
- [Screenshots](#screenshots)
- [How to Run](#how-to-run)
- [Database Schema](#database-schema)
- [To Do](#to-do)
- [License](#license)

---

## ğŸš€ Project Overview

This project automates the extraction, transformation, and loading of real-time laptop listings from Jumia. The scraped data is simultaneously saved into a CSV file and streamed directly into a PostgreSQL data warehouse where it undergoes multiple transformation layers:

- **Bronze Layer**: Raw scraped data
- **Silver Layer**: Cleaned and standardized data via SQL procedures
- **Gold Layer**: Analytical summary tables for reporting

Everything is **Dockerized** and managed using **Apache Airflow**, making it robust, reproducible, and production-ready.

---

## ğŸ§± Architecture

![ETL Architecture Diagram](https://example.com/your-etl-architecture.png)

```text
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚ Web Scraperâ”‚
               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Extract (Python)   â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  CSV File â”‚ â”‚ Bronze DB â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                           â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                   â”‚ Silver Layerâ”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                          â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                   â”‚ Gold Layer â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
             [Analytics, Dashboards, Exports]

  (All steps orchestrated via Docker + Airflow)
```

---

## âš™ï¸ Tech Stack

- **Python** â€“ Web scraping, ETL logic
- **BeautifulSoup & Requests** â€“ For HTML parsing
- **Pandas** â€“ Data manipulation
- **PostgreSQL** â€“ Data warehouse with multi-layer architecture
- **Stored Procedures** â€“ For transformations (Silver & Gold layers)
- **Apache Airflow** â€“ Workflow orchestration
- **Docker** â€“ Containerization
- **Docker Compose** â€“ Multi-service orchestration

---

## âœ¨ Features

- âœ… Real-time scraping of Jumia laptops
- âœ… Dual-loading to CSV and PostgreSQL
- âœ… Layered warehouse: Bronze, Silver, Gold
- âœ… SQL transformations via stored procedures
- âœ… Fully containerized using Docker
- âœ… Scheduled or manual runs via Airflow UI
- âœ… Scalable and modular design

---

## ğŸ“ Project Structure

```bash
jumia-laptop-etl-pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ jumia_etl_dag.py                # Airflow DAG definition
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ extract.py                      # Scrapes Jumia laptops
â”‚   â”œâ”€â”€ transform.py                    # (Optional) Data cleanup
â”‚   â””â”€â”€ load.py                         # Loads data into bronze table
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_bronze_table.sql
â”‚   â”œâ”€â”€ silver_layer_proc.sql
â”‚   â””â”€â”€ gold_layer_proc.sql
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_laptops.csv              # Sample data
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analyze_gold_layer.ipynb        # Optional analytics preview
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ–¼ï¸ Screenshots

### ğŸ“Œ Airflow DAG  
![Airflow DAG Screenshot](https://example.com/airflow-dag.png)

### ğŸ“Œ Airflow Task Logs  
![Airflow Logs Screenshot](https://example.com/airflow-logs.png)

### ğŸ“Œ PostgreSQL Layers  
![DB Tables Screenshot](https://example.com/db-tables.png)

---

## ğŸ§ª How to Run

### ğŸ“¦ Prerequisites
- Docker & Docker Compose installed
- Git

### â–¶ï¸ Run the Pipeline

```bash
# Clone the repository
git clone https://github.com/your-username/jumia-laptop-etl-pipeline.git
cd jumia-laptop-etl-pipeline

# Start all services (Airflow, Postgres)
docker-compose up --build

# Open Airflow UI
# Visit http://localhost:8080
# Login: admin / admin (default)

# Trigger the DAG manually or wait for the daily schedule
```

---

## ğŸ—ƒï¸ Database Schema

### Bronze Table
| Column        | Type      |
|---------------|-----------|
| product_name  | TEXT      |
| new_price     | NUMERIC   |
| old_price     | NUMERIC   |
| discount      | TEXT      |
| scraped_on    | TIMESTAMP |

### Silver Layer
> Cleaned using `silver_layer_proc.sql`: removes nulls, standardizes fields

### Gold Layer
> Aggregated insights created using `gold_layer_proc.sql`: average prices, discounts per brand/category

---

## ğŸ“Œ To Do

- [ ] Add email or Slack alerts on DAG failure
- [ ] Expand scraper to other Jumia product categories
- [ ] Connect to BI tools (Power BI, Metabase)
- [ ] Add unit/integration tests

---

## ğŸ“œ License

This project is licensed under the MIT License. See [LICENSE](https://github.com/your-username/jumia-laptop-etl-pipeline/blob/main/LICENSE) for details.

---

## ğŸ“¬ Contact

Created with â¤ï¸ by [Your Name](https://github.com/your-username)  
Feel free to reach out or open an issue for questions and improvements!
